{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"segmented_char_recog_training.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"MjD9XXYcWzy3","colab_type":"text"},"cell_type":"markdown","source":["# **Libraries**"]},{"metadata":{"id":"xqVlg-LuW2zK","colab_type":"code","colab":{}},"cell_type":"code","source":["import cv2\n","from PIL import ImageFont, ImageDraw, Image \n","import matplotlib.pyplot as plt\n","import os\n","import numpy as np\n","from datetime import datetime as dt\n","import tensorflow as tf\n","import string\n","\n","bold = '\\033[1m'\n","end = '\\033[0m'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RMrAIAR_soab","colab_type":"text"},"cell_type":"markdown","source":["# **Functions**"]},{"metadata":{"id":"4eeygyvpGM9P","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_data(file_list):\n","  '''\n","  read data from tfrecords file\n","  '''\n","  file_queue=tf.train.string_input_producer(file_list)\n","  feature = {'images': tf.FixedLenFeature([], tf.string),\n","             'labels': tf.FixedLenFeature([], tf.string)}    \n","  reader = tf.TFRecordReader()  \n","  _,record=reader.read(file_queue)#read a record\n","  features = tf.parse_single_example(record, features=feature)\n","  img = tf.decode_raw(features['images'], tf.uint8)\n","  label = tf.decode_raw(features['labels'], tf.uint8) \n","  return img,label\n","\n","def minibatch(batch_size, filename, file_count, \\\n","              image_size, max_char, class_count):\n","  '''\n","  create minibatch\n","  '''\n","  file_list=[os.path.join(filename + \\\n","            '%d.tfrecords' % i) for i in range(1, file_count+1)]  \n","  img, label=read_data(file_list)\n","  img = tf.cast(tf.reshape(img,img_size), dtype = tf.float32)\n","  label = tf.reshape(label[0], [1, max_char])# added [0] as workaround, need to resolve the issue\n","  label = tf.one_hot(label[0],class_count,axis=1)# added [0] as workaround, need to resolve the issue\n","  label = tf.reshape(label,tf.shape(label)[1:])\n","  img_batch,label_batch= tf.train.shuffle_batch([img, label],\n","                          batch_size,capacity,min_after_dequeue,\\\n","                          num_threads=num_of_threads)\n","  return img_batch, tf.cast(label_batch, dtype = tf.int64)\n","\n","def variable(name,shape,initializer,weight_decay = None):\n","  '''\n","  create parameter tensor\n","  '''\n","  var = tf.get_variable(name, shape, initializer = initializer)\n","  if weight_decay is not None:\n","    weight_loss = tf.multiply(tf.nn.l2_loss(var),weight_decay, name=\"weight_loss\")\n","    tf.add_to_collection('losses', weight_loss)\n","  return var\n","\n","\n","#need to customize activation and lrn\n","def conv_block(block_num,\n","               input_data,\n","               weights, \n","               weight_initializer=tf.contrib.layers.xavier_initializer(),\n","               bias_initializer=tf.constant_initializer(0.0),\n","               conv_op=[1,1,1,1],\n","               conv_padding='SAME',\n","               weight_decay=None,\n","               lrn=True,\n","               dropout=1.0, \n","               activation=True):\n","  '''\n","  convolutional block\n","  '''\n","  with tf.variable_scope('conv'+ str(block_num), reuse = tf.AUTO_REUSE) as scope:\n","    input_data = tf.nn.dropout(input_data, dropout)\n","    kernel = variable('weights', weights, initializer = weight_initializer, weight_decay = weight_decay)\n","    biases = variable('biases', weights[3], initializer=bias_initializer, weight_decay=None)\n","    conv = tf.nn.conv2d(input_data, kernel, conv_op, padding=conv_padding)\n","    pre_activation = tf.nn.bias_add(conv, biases)\n","    if lrn==True:\n","      pre_activation = tf.nn.lrn(pre_activation, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm')\n","    if activation:\n","      conv_out = tf.nn.relu(pre_activation, name=scope.name)\n","      return conv_out\n","    else:\n","      return pre_activation\n","\n","\n","\n","def dense_block(block_num,\n","                input_data,\n","                neurons,\n","                weight_initializer=tf.contrib.layers.xavier_initializer(),\n","                bias_initializer=tf.constant_initializer(0.0),\n","                weight_decay=None,\n","                activation=True, \n","                dropout=1.0):\n","  '''\n","  Fully connected block\n","  '''\n","  with tf.variable_scope('dense'+ str(block_num), reuse = tf.AUTO_REUSE) as scope:\n","    input_data = tf.nn.dropout(input_data, dropout)\n","    weights = variable('weights', [input_data.shape[1], neurons], \\\n","                       initializer=weight_initializer, weight_decay = weight_decay)\n","    biases = variable('biases', [1,neurons], initializer = bias_initializer, weight_decay = None)\n","    dense = tf.matmul(input_data,weights)+biases\n","    if activation:\n","      dense=tf.nn.relu(dense, name=scope.name)\n","    return dense\n","  \n","  \n","  \n","def loss(logits,labels):\n","\tloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels),name='cross_entropy_loss_mean')\n","\ttf.add_to_collection('losses', loss)\n","\ttotal_loss=tf.add_n(tf.get_collection('losses'), name='total_loss')\n","\ttf.add_to_collection('losses', total_loss)\n","\treturn total_loss\n","\n","\n","  \n","def parameter_update(loss, learning_rate):\n","  '''\n","  optimization and parameter update using adam optimizer\n","  '''\n","  optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n","  for var in tf.trainable_variables():\n","    tf.summary.histogram(var.op.name, var)\n","  return optimizer\n","\n","\n","\n","def accuracy_calc(output, label_batch):\n","  '''\n","  calculate accuracy\n","  '''\n","  correct_prediction = tf.equal(tf.cast(tf.argmax(output, 1),dtype=tf.int32),tf.cast(tf.argmax(label_batch, 1),dtype=tf.int32))\n","  accuracy=tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n","  return accuracy   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"vSyN04QxXjY6","colab_type":"text"},"cell_type":"markdown","source":["# Models"]},{"metadata":{"id":"eKDmEw32coH2","colab_type":"code","colab":{}},"cell_type":"code","source":["def inference(image_batch, class_count, weights, dropout=[1,1,1,1],wd=None):\n","  '''\n","  Forward propagation\n","  '''\n","  i = 0           \n","  conv_op=[[1,1,1,1],[1,1,1,1],[1,1,1,1], [1,1,1,1]]\n","  \n","  conv1 = conv_block(1,image_batch,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n","  i=i+1\n","  pool1=tf.nn.max_pool(conv1, ksize=[1, 4, 4, 1], strides=[1,4,4,1],padding='SAME', name='pool1') #32x32\n","  \n","  conv2 = conv_block(2,pool1,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n","  i=i+1\n","  pool2=tf.nn.max_pool(conv2, ksize=[1, 4, 4, 1], strides=[1,4,4,1],padding='SAME', name='pool2') #8x8\n","  \n","  conv3 = conv_block(3,pool2,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n","  i=i+1\n","  pool3=tf.nn.max_pool(conv3, ksize=[1, 4, 4, 1], strides=[1,4,4,1],padding='SAME', name='pool3') #2x2\n","  \n","  conv4 = conv_block(4,pool3,weights[i], conv_op = conv_op[i], conv_padding='SAME', dropout=dropout[i],weight_decay=wd)\n","  i=i+1\n","  pool4=tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1,2,2,1],padding='SAME', name='pool4')#1x1\n","\n","  flat=tf.reshape(pool4, [tf.shape(image_batch)[0], class_count], name='flat')\n","\t\t\n","  return flat"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WbvHOOcjX3i-","colab_type":"text"},"cell_type":"markdown","source":["# **Training functions**"]},{"metadata":{"id":"6Gh9-CFFX3OE","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(folder_path, train_filename, test_filename, \n","          train_data_count, file_count,\n","          weights, dropout, wd,\n","          img_size, max_char, class_count, \n","          batch_size = 32, learning_rate=0.01, epochs=5, \n","          restore=False, var_lr = [None,None]):\n","  \n","\ttrain_step = train_data_count//batch_size\n","\tstart_time = dt.now()\n","  #build graph\n","\twith tf.Graph().as_default():\n","\t\tx_train, y_train = minibatch(batch_size, train_filename, file_count, img_size, max_char, class_count)    \n","\t\tlogit_train = inference(x_train, class_count, weights, dropout = dropout, wd = wd)\n","\t\tcost = loss(logit_train, y_train)\n","\t\tupdate=parameter_update(cost,learning_rate)\t\n","\t\taccuracy_train = accuracy_calc(logit_train, y_train)\n","    \n","\t\tsaver = tf.train.Saver()    \n","    #start session\n","\t\twith tf.Session() as sess:\n","      #initialize the variables\n","\t\t\tsess.run(tf.global_variables_initializer())\n","\t\t\tsess.run(tf.local_variables_initializer())\n","\t\t\tcoord = tf.train.Coordinator()\n","\t\t\tthreads = tf.train.start_queue_runners(coord=coord) \n","      \n","      #restore the variables\n","\t\t\tif restore == True:\n","\t\t\t\tloader = tf.train.import_meta_graph(checkpoint_restore +'.meta')\n","\t\t\t\tloader.restore(sess, checkpoint_restore)\n","        \n","\t\t\t#train for given number of epochs\n","\t\t\tfor e in range(epochs): \n","\t\t\t\tprint(bold + \"\\nepoch:\" + end, e)\n","\t\t\t\ttrain_epoch_cost = 0\n","\t\t\t\ttrain_epoch_acc = 0\n","        \n","        #train for given number of steps in one epoch\n","\t\t\t\tfor s in range(train_step):\n","\t\t\t\t\t_,train_batch_cost = sess.run([update, cost])\t                  \n","\t\t\t\t\ttrain_epoch_cost += train_batch_cost/train_step         \n","\t\t\t\tprint(bold + \"epoch_cost: \" + end, train_epoch_cost)\n","        \n","        #calculate accuracy of training set\n","\t\t\t\tfor i in range(train_step//5):\n","\t\t\t\t\ttrain_batch_acc = sess.run(accuracy_train)\n","\t\t\t\t\ttrain_epoch_acc = train_epoch_acc + (train_batch_acc/(train_step//5))      \n","\t\t\t\tprint(bold + \"train epoch accuracy: \" + end, train_epoch_acc, \"\\n\")\n","        \n","        #afer every lr[0] epoch decrease learning rate by factor of lr[1]\n","\t\t\t\tif var_lr[0] != None:\n","\t\t\t\t\tif e%var_lr[0] == 0:\n","\t\t\t\t\t\tlearning_rate = learning_rate/var_lr[1]\n","\t\t\t\t\n","#         if(e%10 == 0 and e!=0):\n","# # \t\t\t\t\t\tsave_path = saver.save(sess, checkpoint_save)\n","# \t\t\t\t\t\tprint()\n","          \n","      #save all the variables\n","\t\t\tprint(\"creating checkpoint...\")\n","\t\t\tsave_path = saver.save(sess, checkpoint_save)\t\n","\t\t\tprint(\"checkpoint created at \", checkpoint_save)\n","\t\t\tcoord.request_stop()\n","\t\t\tcoord.join(threads)\n","\t\t\tend_time = dt.now()\n","\t\t\tprint(\"total time taken =\", end_time - start_time)\n","\treturn None\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hM-m6EmVscxF","colab_type":"text"},"cell_type":"markdown","source":["# **Hyper-Parameters**"]},{"metadata":{"id":"4STSFeZLVVr1","colab_type":"code","colab":{}},"cell_type":"code","source":["path = \"drive/share/OCR/OCR:Part3/\"\n","data_folder = path + \"dataset/tfrecords/\"\n","\n","max_char = 1\n","img_size = [128,128,1]\n","\n","# digi, lc, sel uc, sel sign\n","checkpoint_restore = path + \"checkpoints/checkpoint_digi_lc_sel_uc_sel_sign_1.ckpt\"\n","checkpoint_save = path + \"checkpoints/checkpoint_digi_lc_sel_uc_sel_sign_1.ckpt\"\n","class_count = 56\n","keyword = 'digi_lc_sel_uc_sel_sign'\n","file_count = 1\n","train_data_count = 31360\n","batch_size = 1120\n","weights=[[3,3,1,16],\n","         [3,3,16,24],\n","         [3,3,24,42], \n","         [3,3,42,56]]\n","\n","\n","train_filename =data_folder + 'dataset_' + keyword + '_'\n","test_filename = data_folder + 'dataset_' + keyword + '_'\n","\n","dropout = [1, 1, 1, 1]\n","wd = 0.0\n","\n","lr = 0.001\n","epochs = 10\n","\n","num_of_threads=16\n","min_after_dequeue=10000\n","capacity= capacity=min_after_dequeue+(num_of_threads+1)*batch_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ICJa2esMYIBK","colab_type":"text"},"cell_type":"markdown","source":["# **Train**"]},{"metadata":{"id":"VbnWr-Agz85f","colab_type":"code","colab":{}},"cell_type":"code","source":["train(folder_path, train_filename, test_filename, \n","      train_data_count, file_count,\n","      weights, dropout, wd,\n","      img_size, max_char, class_count,\n","      batch_size=batch_size, learning_rate=lr, epochs=epochs, \n","      restore=False, var_lr=[None,None])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Frpq81Igs_8I","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}